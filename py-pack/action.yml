name: 'py-pack'
description: 'Package RC for this service'
outputs:
  service-name:
    description: "Service Name"
    value: ${{ steps.packer.outputs.service_name }}
  s3-prefix:
    description: "S3 key prefix where the RC was uploaded"
    value: ${{ steps.packer.outputs.s3_prefix }}
  s3-bucket:
    description: "S3 bucket to where the RC was uploaded"
    value: ${{ steps.packer.outputs.s3_bucket }}
  version:
    description: "RC version"
    value: ${{ steps.packer.outputs.rc_version }}
  library-version:
    description: "Library version"
    value: ${{ steps.packer.outputs.library_version }}
runs:
  using: "composite"
  steps:
    - run: |

          # Configure pip to use code artifact as additional pypi
          ca_domain=agwafarm-private
          ca_repo=agwafarm-private
          ca_account=953022346399
          ca_token=$(aws codeartifact get-authorization-token --domain $ca_domain --domain-owner $ca_account --query authorizationToken --output text)
          ca_region=us-west-2
          aws codeartifact login --tool twine --repository $ca_repo --domain $ca_domain --domain-owner $ca_account
          pip config set global.extra-index-url https://aws:$ca_token@$ca_domain-$ca_account.d.codeartifact.$ca_region.amazonaws.com/pypi/$ca_repo/simple/
          
          # Compute useful variables
          service_name=$GITHUB_REPOSITORY
          service_name=$(echo $service_name | sed -e 's/^agwafarm\///')
          service_name=$(echo $service_name | sed -e 's/^agwa\-//')

          if [ "$service_name" = "cloud-components" ]; then
             service_name=cloud-parent
          fi
          if [ "$service_name" = "greengrass-service" ]; then
             service_name=greengrass-parent
          fi

          echo packing service $service_name
          echo "::set-output name=service_name::$service_name"

          rc_version=$GITHUB_SHA
          echo RC version $rc_version
          echo ::set-output name=rc_version::$rc_version

          event_name=$GITHUB_EVENT_NAME
          echo github event name $event_name

          if [ "$event_name" = "pull_request" ]; then
             git_ref=$GITHUB_HEAD_REF
          else
             git_ref=$GITHUB_REF
          fi
          echo git ref $git_ref

          branch_name=$(echo $git_ref | sed -e 's/^refs\/heads\///')
          echo branch name $branch_name

          if [ "$branch_name" = "main" ] || [ "$branch_name" = "master" ]; then
             export AGWA_SERVICE_LIBRARY_TAG=latest
             s3_retainment=standard
          else
             user_name=$(echo $GITHUB_ACTOR | sed -e 's/[^[:alnum:]]+/_/g')
             user_name=$(echo $user_name | sed -e 's/\(.*\)/\L\1/')
             echo user name $user_name
             export AGWA_SERVICE_LIBRARY_TAG=edge-$user_name
             s3_retainment=low
          fi

          echo using library tag $AGWA_SERVICE_LIBRARY_TAG
          echo ::set-output name=library_version::$AGWA_SERVICE_LIBRARY_TAG
          
          s3_bucket=agwa-ci-assets
          s3_prefix=$s3_retainment/$service_name/$rc_version
          s3_path_base=s3://$s3_bucket/$s3_prefix
          echo s3 bucket $s3_bucket
          echo s3 path base $s3_path_base

          current_folder=$(pwd)
          common_layer_path=artifacts/layers/common
          asset_name=asset.zip

          if [ -f "package-lock.json" ]; then
              npm ci
          elif [ -f "package.json" ]; then
              npm i
          fi

          #Synthesize CDK apps into cloudformation templates.
          if [ -f "cdk.json" ]; then
              npm i -g aws-cdk@1.94.1
              mkdir src/cloudformation
              export APP_COMPANY_NAME=agwa
              export APP_STACK=$service_name
              export APP_SERVICE=$service_name
              export APP_CORS_ORIGIN="*"
              export APP_CORS_HEADERS="Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token,X-Amz-User-Agent,Accept,User-Agent,Referer"
              export APP_CORS_METHODS="OPTIONS,GET,PUT,POST,DELETE,PATCH,HEAD"
              cdk synthesize --no-version-reporting --asset-metadata false --path-metadata false $APP_STACK > src/cloudformation/main.yaml
          fi

          #Upload CloudFormation Assets, delete files in target folder if they no longer exist.
          aws s3 sync --delete src/cloudformation $s3_path_base/cloudformation
          
          mkdir -p $common_layer_path
          
          if [ -f "requirements.txt" ]; then
            python3 -m pip install --upgrade pip==20.2.3
            python3 -m pip install --upgrade --force-reinstall wheel==0.38.4
            python3 -m pip install --upgrade --force-reinstall setuptools==65.7.0
            python3 -m pip install -q -r requirements.txt --verbose
            # Create and Upload Lambda & Layer Assets 
            common_layer_target_path=$common_layer_path/python
            python3 -m pip install -q -r requirements.txt --target $common_layer_target_path
          fi

          if [ -f "requirements-dev.txt" ]; then
            python3 -m pip install --upgrade pip==20.2.3
            python3 -m pip install --upgrade --force-reinstall wheel==0.38.4
            python3 -m pip install --upgrade --force-reinstall setuptools==65.7.0
            python3 -m pip install -q -r requirements-dev.txt
          fi
          
          # Copy local common files to layer before zipping
          if [ -d "src/common" ] ; then
            cp -r src/common $common_layer_target_path
          fi

          # upload cloud-common folder if not empty
          if [ "$(ls -A $common_layer_path)" ]; then
              cd $common_layer_path
              zip -q -r $asset_name .
              aws s3 cp $asset_name $s3_path_base/layers/cloud-common.zip
              cd $current_folder 
          fi


          if [ "$service_name" = "greengrass-parent" ]; then
            echo "Uploading Greengrass Service"
            
            echo "Uploading General Lambdas"
            for d in src/lambdas/general/*/ ; do
              func_name=$(basename "$"$d"")
              func_s3_key=$s3_path_base/functions/${func_name}.zip
              pushd src/lambdas/general/${func_name}
              zip -r ../../../../artifacts/${func_name}.zip *.py **/*.py
              popd
              aws s3 cp artifacts/${func_name}.zip $func_s3_key
            done
            
            echo "Uploading Sensor Lambdas"
            for d in src/lambdas/sensors/functions/*/ ; do
              func_name=$(basename "$"$d"")
              func_s3_key=$s3_path_base/functions/${func_name}.zip
              zip -j artifacts/${func_name}.zip src/lambdas/sensors/functions/${func_name}/*.py
              pushd src/lambdas/sensors
              zip -ur ../../../artifacts/${func_name}.zip common/*.py
              popd
              aws s3 cp artifacts/${func_name}.zip $func_s3_key
            done
            
            echo "Uploading Actuator Lambdas"
            for d in src/lambdas/actuators/functions/*/ ; do
              func_name=$(basename "$"$d"")
              func_s3_key=$s3_path_base/functions/${func_name}.zip
              zip -j artifacts/${func_name}.zip src/lambdas/actuators/functions/${func_name}/*.py
              pushd src/lambdas/actuators
              zip -ur ../../../artifacts/${func_name}.zip common/*.py
              popd
              aws s3 cp artifacts/${func_name}.zip $func_s3_key
            done
          else
            for d in src/lambdas/*/ ; do
              func_name=$(basename "$"$d"")
              func_path=src/lambdas/${func_name}
              func_artifact_folder=artifacts/lambdas/${func_name}
              func_s3_key=$s3_path_base/functions/${func_name}.zip
              req_path=src/lambdas/${func_name}/requirements.txt
              layer_path=artifacts/layers/lambdas/${func_name}
              layer_s3_key=$s3_path_base/layers/${func_name}.zip
              
              cd $current_folder
              mkdir -p $func_artifact_folder
              
              #Lambda has specific dependencies
              if [ -f $req_path ]; then
                mkdir -p $layer_path
                target_path=$layer_path/python
                python3 -m pip install -q -r $req_path --target $target_path
                python3 -m pip install -q -r $req_path
                cd $layer_path
                zip -q -r $asset_name .
                aws s3 cp $asset_name $layer_s3_key
                cd $current_folder
              fi
              
              cd $func_path
              zip -q -r $asset_name .
              aws s3 cp $asset_name $func_s3_key
              cd $current_folder
            done
          fi

      shell: bash
      id: packer