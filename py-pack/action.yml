name: "py-pack"
description: "Package RC for this service"
outputs:
  service-name:
    description: "Service Name"
    value: ${{ steps.packer.outputs.service_name }}
  s3-prefix:
    description: "S3 key prefix where the RC was uploaded"
    value: ${{ steps.packer.outputs.s3_prefix }}
  s3-bucket:
    description: "S3 bucket to where the RC was uploaded"
    value: ${{ steps.packer.outputs.s3_bucket }}
  version:
    description: "RC version"
    value: ${{ steps.packer.outputs.rc_version }}
inputs:
  hotfix:
    description: "whether it is an hotfix version or not"
    required: false
    default: "false"
  environment:
    description: "Deployment environment"
    required: true

runs:
  using: "composite"
  steps:
    - run: |
        # Compute useful variables
        service_name=$GITHUB_REPOSITORY
        service_name=$(echo $service_name | sed -e 's/^agwafarm\///')
        service_name=$(echo $service_name | sed -e 's/^agwa\-//')

        if [ "$service_name" = "cloud-components" ]; then
           service_name=cloud-parent
        fi
        if [ "$service_name" = "greengrass-service" ]; then
           service_name=greengrass-parent
        fi

        echo Environment: ${{ inputs.environment }}

        echo packing service $service_name
        echo "::set-output name=service_name::$service_name"

        rc_version=$GITHUB_SHA
        echo RC version $rc_version
        echo ::set-output name=rc_version::$rc_version
             
        event_name=$GITHUB_EVENT_NAME
        echo github event name $event_name

        if [ "$event_name" = "pull_request" ]; then
           git_ref=$GITHUB_HEAD_REF
        else
           git_ref=$GITHUB_REF
        fi
        echo git ref $git_ref

        branch_name=$(echo $git_ref | sed -e 's/^refs\/heads\///')
        echo branch name $branch_name

        if [ "$branch_name" = "main" ] || [ "$branch_name" = "master" ] || [ "$branch_name" = "hotfix/water_level" ] || [ "${{ inputs.hotfix }}" = "true" ]; then
          s3_retainment=standard
          target_env=ci
        else
          user_name=$(echo $GITHUB_ACTOR | sed -e 's/[^[:alnum:]]+/_/g')
          user_name=$(echo $user_name | sed -e 's/\(.*\)/\L\1/')
          echo user name $user_name
          s3_retainment=low
          target_env="dev${user_name}"
        fi

        s3_bucket=agwa-ci-assets
        s3_prefix=$s3_retainment/$service_name/$rc_version
        s3_path_base=s3://$s3_bucket/$s3_prefix
        echo s3 bucket $s3_bucket
        echo s3 path base $s3_path_base

        current_folder=$(pwd)
        common_layer_path=artifacts/layers/common
        asset_name=asset.zip

        if [ -f "package-lock.json" ]; then
            npm ci
        elif [ -f "package.json" ]; then
            npm i
        fi

        # Synthesize CDK apps into cloudformation templates.
        if [ -f "cdk.json" ]; then
            npm i -g aws-cdk@1.94.1
            mkdir src/cloudformation
            export APP_COMPANY_NAME=agwa
            export APP_STACK=$service_name
            export APP_SERVICE=$service_name
            export APP_CORS_ORIGIN="*"
            export APP_CORS_HEADERS="Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token,X-Amz-User-Agent,Accept,User-Agent,Referer"
            export APP_CORS_METHODS="OPTIONS,GET,PUT,POST,DELETE,PATCH,HEAD"
            cdk synthesize --no-version-reporting --asset-metadata false --path-metadata false $APP_STACK > src/cloudformation/main.yaml
        fi

        # Upload CloudFormation Assets, delete files in target folder if they no longer exist.
        aws s3 sync --delete src/cloudformation $s3_path_base/cloudformation

        mkdir -p $common_layer_path

        if [ -f "requirements.txt" ]; then
          python3 -m pip install --upgrade pip
          python3 -m pip install -q -r requirements.txt --verbose
          # Create and Upload Lambda & Layer Assets 
          common_layer_target_path=$common_layer_path/python
          python3 -m pip install -q -r requirements.txt --target $common_layer_target_path
        fi

        if [ -f "requirements-dev.txt" ]; then
          python3 -m pip install --upgrade pip
          python3 -m pip install git+https://github.com/aws-greengrass/aws-greengrass-gdk-cli.git@v1.6.2
          python3 -m pip install -q -r requirements-dev.txt
        fi

        # Copy local common files to layer before zipping
        if [ "$service_name" != "greengrass-parent" ] && [ -d "src/common" ]; then
          cp -r src/common $common_layer_target_path
        fi

        # upload cloud-common folder if not empty
        if [ "$(ls -A $common_layer_path)" ]; then
            cd $common_layer_path
            zip -q -r $asset_name .
            aws s3 cp $asset_name $s3_path_base/layers/cloud-common.zip
            cd $current_folder 
        fi

        if [ "$service_name" = "greengrass-parent" ]; then
          echo "------------ Uploading Greengrass V1 Service ------------"

          echo "Uploading General Lambdas"
          for d in src/gg1/lambdas/general/*/ ; do
            func_name=$(basename "$d")
            func_s3_key=$s3_path_base/functions/${func_name}.zip
            pushd "src/gg1/lambdas/general/${func_name}"
            mkdir -p general/${func_name}
            cp -r ../../../../common/*.py .
            cp -r ../../../../common/utils .
            cp -r ../../../../common/general/${func_name}/ general/
            zip -r ../../../../../artifacts/${func_name}.zip .
            popd
            aws s3 cp artifacts/${func_name}.zip $func_s3_key
          done

          echo "Uploading Sensor Lambdas"
          for d in src/gg1/lambdas/sensors/*/ ; do
            func_name=$(basename "$d")
            func_s3_key=$s3_path_base/functions/${func_name}.zip
            pushd "src/gg1/lambdas/sensors/${func_name}"
            mkdir -p sensors/${func_name}
            cp -r ../../../../common/*.py .
            cp -r ../../../../common/utils .
            cp -r ../../../../common/sensors/common sensors
            cp -r ../../../../common/sensors/${func_name}/* sensors/${func_name}
            zip -r ../../../../../artifacts/${func_name}.zip .
            popd
            aws s3 cp artifacts/${func_name}.zip $func_s3_key
          done

          echo "Uploading Actuator Lambdas"
          for d in src/gg1/lambdas/actuators/*/ ; do
            func_name=$(basename "$d")
            func_s3_key=$s3_path_base/functions/${func_name}.zip
            pushd "src/gg1/lambdas/actuators/${func_name}"
            mkdir -p actuators/${func_name}
            cp -r ../../../../common/*.py .
            cp -r ../../../../common/utils .
            cp -r ../../../../common/actuators/common actuators
            cp -r ../../../../common/actuators/${func_name}/* actuators/${func_name}
            zip -r ../../../../../artifacts/${func_name}.zip .
            popd
            aws s3 cp artifacts/${func_name}.zip $func_s3_key
          done

          echo "------------ Uploading Greengrass V2 Service ------------"

          declare -a envs_arr=("$target_env")
          if [ "$s3_retainment" = "standard" ]; then
            envs_arr+=("test" "prod")
          fi

          current_path=$(pwd)
          echo "creating temporary dir at $current_path"
          mkdir -p $current_path/temp
          
          for build_env in "${envs_arr[@]}"; do
            echo "---- uploading to $build_env env ----"
            s3_gg2_path_base="${build_env}-agwa-gg2-agwa-components-bucket"
            agwa_m_app_bucket="${build_env}-agwa-m-app"
            echo s3 gg2 components bucket $s3_gg2_path_base

            components=($(ls -d src/gg2/components/* | xargs -n 1 basename))

            for component_name in "${components[@]}"; do
              echo "Uploading $component_name component"

              pushd "src/gg2/components/${component_name}"
              common_path=../../../common
              
              dir_name=$component_name
              if [ "$component_name" = "policy_manager" ]; then
                dir_name=general
              fi

              echo "component_name: $component_name"
              if [ "$component_name" = "agwa_m_web_app" ]; then
                echo "Downloading Agwa M web app build files"
                aws s3 cp s3://${agwa_m_app_bucket}/ web_app_build --recursive
              else
                echo "Copying common greengrass files"
                cp -r $common_path/*.py .
                cp -r $common_path/utils .
                rm -f greengrass_manager_v1.py

                if [ -d $common_path/$dir_name ]; then
                  echo "Copying $component_name component common files to $dir_name folder"
                  mkdir -p $dir_name
                  cp -r $common_path/$dir_name/* $dir_name
                fi
              fi

              cp recipe.yaml $current_path/temp/recipe.yaml
              cp gdk-config.json $current_path/temp/gdk-config.json

              echo "Replacing place holders in recipe.yaml and gdk-config.json"              
              perl -pi -e "s/\+\+place_holder\+\+/${build_env}/g" recipe.yaml
              perl -pi -e "s/\{ENV\}/${build_env}/g" gdk-config.json
              jq '.component."'$build_env'_'$component_name'" = .component.place_holder | del(.component.place_holder)' gdk-config.json > tmp.json && mv tmp.json gdk-config.json
              
              gdk component build
              gdk component publish -b $s3_gg2_path_base

              echo "put back original recipe and gdk-config files"
              cp $current_path/temp/recipe.yaml recipe.yaml
              cp $current_path/temp/gdk-config.json gdk-config.json
              popd
            done
          done

          rm -rf $current_path/temp

        else
          for d in src/lambdas/*/ ; do
            func_name=$(basename "$d")
            func_path=src/lambdas/${func_name}
            func_artifact_folder=artifacts/lambdas/${func_name}
            func_s3_key=$s3_path_base/functions/${func_name}.zip
            req_path=src/lambdas/${func_name}/requirements.txt
            layer_path=artifacts/layers/lambdas/${func_name}
            layer_s3_key=$s3_path_base/layers/${func_name}.zip

            cd $current_folder
            mkdir -p $func_artifact_folder

            # Lambda has specific dependencies
            if [ -f $req_path ]; then
              mkdir -p $layer_path
              target_path=$layer_path/python
              python3 -m pip install -q -r $req_path --target $target_path
              python3 -m pip install -q -r $req_path
              cd $layer_path
              zip -q -r $asset_name .
              aws s3 cp $asset_name $layer_s3_key
              cd $current_folder
            fi

            cd $func_path
            zip -q -r $asset_name .
            aws s3 cp $asset_name $func_s3_key
            cd $current_folder
          done
        fi
      shell: bash
      id: packer
