name: 'py-pack'
description: 'Package RC for this service'
outputs:
  service-name:
    description: "Service Name"
    value: ${{ steps.packer.outputs.service_name }}
  environment:
    description: "Environment name"
    value: ${{ steps.packer.outputs.target_env }}
  s3-prefix:
    description: "S3 key prefix where the RC was uploaded"
    value: ${{ steps.packer.outputs.s3_prefix }}
  s3-bucket:
    description: "S3 bucket to where the RC was uploaded"
    value: ${{ steps.packer.outputs.s3_bucket }}
  version:
    description: "RC version"
    value: ${{ steps.packer.outputs.rc_version }}
runs:
  using: "composite"
  steps:
    - run: |

          # Configure pip to use code artifact as additional pypi
          ca_domain=agwafarm-private
          ca_repo=agwafarm-private
          ca_account=953022346399
          ca_token=$(aws codeartifact get-authorization-token --domain $ca_domain --domain-owner $ca_account --query authorizationToken --output text)
          ca_region=us-west-2
          aws codeartifact login --tool twine --repository $ca_repo --domain $ca_domain --domain-owner $ca_account
          pip config set global.extra-index-url https://aws:$ca_token@$ca_domain-$ca_account.d.codeartifact.$ca_region.amazonaws.com/pypi/$ca_repo/simple/
          
          # Compute useful variables
          service_name=$GITHUB_REPOSITORY
          service_name=$(echo $service_name | sed -e 's/^agwafarm\///')
          service_name=$(echo $service_name | sed -e 's/^agwa\-//')
          service_name=$(echo $service_name | sed -e 's/\-service$//')

          echo packing service $service_name
          echo "::set-output name=service_name::$service_name"

          rc_version=$GITHUB_SHA
          echo RC version $rc_version
          echo ::set-output name=rc_version::$rc_version

          event_name=$GITHUB_EVENT_NAME
          echo github event name $event_name

          if [ "$event_name" = "pull_request" ]; then
             git_ref=$GITHUB_HEAD_REF
          else
             git_ref=$GITHUB_REF
          fi
          echo git ref $git_ref

          branch_name=$(echo $git_ref | sed -e 's/^refs\/heads\///')
          echo branch name $branch_name

          if [ "$branch_name" = "main" ] || [ "$branch_name" = "master" ]; then
             export AGWA_SERVICE_LIBRARY_TAG=latest
             target_env=ci
          else
             user_name=$(echo $GITHUB_ACTOR | sed -e 's/[^[:alnum:]]+/_/g')
             user_name=$(echo $user_name | sed -e 's/\(.*\)/\L\1/')
             echo user name $user_name              
             export AGWA_SERVICE_LIBRARY_TAG=edge-$user_name
             target_env=dev$user_name
          fi

          echo ::set-output name=target_env::$target_env
          
          s3_bucket=agwa-ci-assets
          s3_prefix=$service_name/$rc_version
          s3_path_base=s3://$s3_bucket/$s3_prefix
          echo s3 bucket $s3_bucket
          echo s3 path base $s3_path_base

          current_folder=$(pwd)
          common_layer_path=artifacts/layers/common
          asset_name=asset.zip

          if [ -f "package-lock.json" ]; then
              npm ci
          elif [ -f "package.json" ]; then
              npm i
          fi

          #Synthesize CDK apps into cloudformation templates.
          if [ -f "cdk.json" ]; then
              npm i -g aws-cdk@1.94.1
              mkdir src/cloudformation
              export APP_STACK=$service_name
              export CORS_ORIGIN="*"
              export CORS_HEADERS="Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token,X-Amz-User-Agent,Accept,User-Agent,Referer"
              export CORS_METHODS="OPTIONS,GET,PUT,POST,DELETE,PATCH,HEAD"
              cdk synthesize --no-version-reporting --asset-metadata false --path-metadata false $APP_STACK > src/cloudformation/main.yaml
          fi

          #Upload CloudFormation Assets, delete files in target folder if they no longer exist.
          aws s3 sync --delete src/cloudformation $s3_path_base/cloudformation
          
          # Create and Upload Lambda & Layer Assets
          mkdir -p $common_layer_path
          common_layer_target_path=$common_layer_path/python
          python3 -m pip install -q -r requirements.txt --target $common_layer_target_path
          python3 -m pip install -q -r requirements.txt

          # Common Lambda Layer
          cd $common_layer_path
          zip -q -r $asset_name .
          aws s3 cp $asset_name $s3_path_base/layers/cloud-common.zip
          cd $current_folder

          for d in src/lambdas/*/ ; do
            func_name=$(basename "$"$d"")
            func_path=src/lambdas/${func_name}
            func_artifact_folder=artifacts/lambdas/${func_name}
            func_s3_key=$s3_path_base/functions/${func_name}.zip
            req_path=src/lambdas/${func_name}/requirements.txt
            layer_path=artifacts/layers/lambdas/${func_name}
            layer_s3_key=$s3_path_base/layers/${func_name}.zip
            
            cd $current_folder
            mkdir -p $func_artifact_folder
            
            #Lambda has specific dependencies
            if [ -f $req_path ]; then
              mkdir -p $layer_path
              target_path=$layer_path/python
              python3 -m pip install -q -r $req_path --target $target_path
              python3 -m pip install -q -r $req_path
              cd $layer_path
              zip -q -r $asset_name .
              aws s3 cp $asset_name $layer_s3_key
              cd $current_folder
            fi
            
            cd $func_path
            zip -q -r $asset_name .
            aws s3 cp $asset_name $func_s3_key
            cd $current_folder
          done

      shell: bash
      id: packer